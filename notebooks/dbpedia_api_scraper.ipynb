{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "output_folder = '../dbpedia/' #where we store the json files we retrieve from the sparql endpoint\n",
    "corpus_folder = '../data/corpora/' #where we output the text files we create\n",
    "\n",
    "def query_dbpedia(query):\n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setQuery(query) \n",
    "    return sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectnames = ['Device', 'Animal', 'Biomolecule', 'Company', 'ChemicalSubstance', 'Food', 'Plant', 'MeanOfTransportation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11957 results found for Device\n",
      "229955 results found for Animal\n",
      "7335 results found for Biomolecule\n",
      "107357 results found for Company\n",
      "18505 results found for ChemicalSubstance\n",
      "11358 results found for Food\n",
      "62277 results found for Plant\n",
      "55325 results found for MeanOfTransportation\n"
     ]
    }
   ],
   "source": [
    "for objectname in objectnames: \n",
    "    #get the count of objects for this query\n",
    "    query = \"\"\"PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        PREFIX n1: <http://schema.org/>\n",
    "        SELECT COUNT DISTINCT ?Device ?abstract\n",
    "        WHERE { ?Device a dbo:\"\"\"+objectname+\"\"\" .\n",
    "                ?Device dbo:abstract ?abstract . \n",
    "                filter(langMatches(lang(?abstract),\"en\"))}\"\"\"\n",
    "    res = query_dbpedia(query)\n",
    "    res = res['results']['bindings'][0]['callret-0'].get('value')\n",
    "    print(str(res)+ \" results found for \"+objectname)\n",
    "    \n",
    "    #figure out how often we need to query, because the endpoint is limited to 10000 at a time. \n",
    "    num = math.ceil(int(res) / 10000)\n",
    "    \n",
    "    for i in range(num):\n",
    "        query = \"\"\"PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        PREFIX n1: <http://schema.org/>\n",
    "        SELECT DISTINCT ?Device ?abstract\n",
    "        WHERE { ?Device a dbo:\"\"\"+objectname+\"\"\" .\n",
    "                ?Device dbo:abstract ?abstract . \n",
    "                filter(langMatches(lang(?abstract),\"en\"))}\n",
    "        LIMIT 10000\n",
    "        OFFSET \"\"\"+str(i*10000)\n",
    "        #print(query_dbpedia(query))\n",
    "        name = output_folder + objectname + str(i)+'.json'\n",
    "        with open(name, 'w') as outfile:\n",
    "            json.dump(query_dbpedia(query), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For good measure, we also get the schema.org product - note this includes the productontology stuff to some degree- \n",
    "\n",
    "objectname = \"Product\"\n",
    "#get the count of objects for this query\n",
    "query = \"\"\"PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        PREFIX n1: <http://schema.org/>\n",
    "        SELECT COUNT DISTINCT ?Device ?abstract\n",
    "        WHERE { ?Device a n1:Product .\n",
    "                ?Device dbo:abstract ?abstract . \n",
    "                filter(langMatches(lang(?abstract),\"en\"))}\"\"\"\n",
    "res = query_dbpedia(query)\n",
    "res = res['results']['bindings'][0]['callret-0'].get('value')\n",
    "print(str(res)+ \" results found for \"+objectname)\n",
    "num = math.ceil(int(res) / 10000)\n",
    "    \n",
    "for i in range(num):\n",
    "    q = \"\"\"\n",
    "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "    PREFIX n1: <http://schema.org/>\n",
    "    SELECT DISTINCT ?Device ?abstract\n",
    "    WHERE { ?Device a n1:\"\"\"+objectname+\"\"\" .\n",
    "            ?Device dbo:abstract ?abstract . \n",
    "            filter(langMatches(lang(?abstract),\"en\"))}\n",
    "            LIMIT 10000\n",
    "            OFFSET \"\"\"+str(i*10000)\n",
    "\n",
    "    name = output_folder + objectname + str(i)+'.json'\n",
    "    with open(name, 'w') as outfile:\n",
    "        json.dump(query_dbpedia(q), outfile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we got a ton of json files, whats next?\n",
    "# start looping through the directory, and output everything found to a list named files\n",
    "files = [f for f in os.listdir(output_folder) if os.path.isfile(os.path.join(output_folder, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_cleaner(raw):\n",
    "    # we definitely need to conserve hyphens! thus, we do not include these in this expression\n",
    "    return re.sub(\"[^a-zA-Z0-9-]\",\" \", raw)\n",
    "\n",
    "def comma_splitter(text):\n",
    "    text = text.replace(';', '')\n",
    "    text = text.replace('\"', '')\n",
    "    return text.replace(',', '')\n",
    "\n",
    "def spurious_whitespace_remover(text):\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31277 lines found in Animal0.json\n",
      "22504 lines found in Animal1.json\n",
      "25982 lines found in Animal10.json\n",
      "33491 lines found in Animal11.json\n",
      "30319 lines found in Animal12.json\n",
      "27903 lines found in Animal13.json\n",
      "41913 lines found in Animal14.json\n",
      "34643 lines found in Animal15.json\n",
      "24962 lines found in Animal16.json\n",
      "27349 lines found in Animal17.json\n",
      "27979 lines found in Animal18.json\n",
      "33899 lines found in Animal19.json\n",
      "37647 lines found in Animal2.json\n",
      "24694 lines found in Animal20.json\n",
      "29925 lines found in Animal21.json\n",
      "35480 lines found in Animal22.json\n",
      "34841 lines found in Animal3.json\n",
      "24670 lines found in Animal4.json\n",
      "29925 lines found in Animal5.json\n",
      "29254 lines found in Animal6.json\n",
      "32562 lines found in Animal7.json\n",
      "28415 lines found in Animal8.json\n",
      "38343 lines found in Animal9.json\n",
      "32314 lines found in Biomolecule0.json\n",
      "41691 lines found in ChemicalSubstance0.json\n",
      "37994 lines found in ChemicalSubstance1.json\n",
      "48707 lines found in Company0.json\n",
      "53449 lines found in Company1.json\n",
      "36755 lines found in Company10.json\n",
      "49120 lines found in Company2.json\n",
      "43978 lines found in Company3.json\n",
      "51320 lines found in Company4.json\n",
      "52620 lines found in Company5.json\n",
      "44555 lines found in Company6.json\n",
      "46733 lines found in Company7.json\n",
      "49767 lines found in Company8.json\n",
      "46529 lines found in Company9.json\n",
      "54952 lines found in Device0.json\n",
      "9540 lines found in Device1.json\n",
      "54904 lines found in Food0.json\n",
      "6750 lines found in Food1.json\n",
      "46940 lines found in MeanOfTransportation0.json\n",
      "45200 lines found in MeanOfTransportation1.json\n",
      "46190 lines found in MeanOfTransportation2.json\n",
      "45146 lines found in MeanOfTransportation3.json\n",
      "47846 lines found in MeanOfTransportation4.json\n",
      "22098 lines found in MeanOfTransportation5.json\n",
      "56372 lines found in Plant0.json\n",
      "46022 lines found in Plant1.json\n",
      "53331 lines found in Plant2.json\n",
      "50226 lines found in Plant3.json\n",
      "55045 lines found in Plant4.json\n",
      "40950 lines found in Plant5.json\n",
      "8606 lines found in Plant6.json\n",
      "46162 lines found in Product0.json\n",
      "44940 lines found in Product1.json\n",
      "44894 lines found in Product2.json\n",
      "46564 lines found in Product3.json\n",
      "46306 lines found in Product4.json\n",
      "25020 lines found in Product5.json\n"
     ]
    }
   ],
   "source": [
    "total_lines = 0\n",
    "total_words = 0\n",
    "for textfile in files:\n",
    "    with open(output_folder+textfile) as file:\n",
    "            data = json.load(file)\n",
    "            corpus = [] #wont go over 10000 items per file, so its OK to build corpora in memory IMO\n",
    "            for idx, item in enumerate(data['results']['bindings']):\n",
    "                #print(item)\n",
    "                abstract  = item['abstract']['value']\n",
    "                for l in sent_tokenize(abstract):\n",
    "                #for l in re.split(r\"(\\. |\\? |\\! )\",abstract): #split on punctuation symbols\n",
    "                    if (len(l) > 3):\n",
    "                        l = l.rstrip(\"\\n\\r\") #remove newlines and other gibberish\n",
    "                        l = unidecode.unidecode(l).lower()\n",
    "                        l = comma_splitter(l)\n",
    "                        l = sentence_cleaner(l)\n",
    "                        l = spurious_whitespace_remover(l)\n",
    "                        corpus.append(l)\n",
    "                        total_words += len(l.split(\" \"))\n",
    "                        \n",
    "            #save our corpus to disk\n",
    "            print(str(len(corpus))+\" lines found in \"+textfile)\n",
    "            total_lines += len(corpus)\n",
    "            \n",
    "            with open(corpus_folder+textfile.split('.')[0]+\".txt\", \"w\") as f:\n",
    "                for item in corpus:\n",
    "                    f.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the second uss bainbridge dd-1 was the first destroyer in the united states navy and the lead ship of the bainbridge-class'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40916158"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words #how many extra words we can use to train our word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2287513"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_lines #how many extra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
