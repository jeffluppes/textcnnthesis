{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN training\n",
    "This is an updated and slightly cleaned version of the notebook I used to train my CNN as proposed in my thesis (see the pdf). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name used for saving models between epochs, etc. Should be a unique identifier for this notebook. \n",
    "experiment_name = \"example_cnn_showcase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the seed first to not introduce any randomness during imports\n",
    "# this should in theory mean our results are reproducible\n",
    "# however, there seems to be some randomness when using a GPU due to cuDNN\n",
    "# see https://github.com/keras-team/keras/issues/2479\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Keras - todo, refactor these imports\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import concatenate, BatchNormalization\n",
    "\n",
    "# Gensim NLP library (word embeddings and such)\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for storing and loading objects\n",
    "from joblib import dump, load\n",
    "\n",
    "#labelencoding\n",
    "from sklearn import preprocessing \n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# F1-score and friends\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "\n",
    "# How long the sentences are going to be. Longer sentences are cut off, shorter are padded\n",
    "MAX_SEQUENCE_LENGTH = 16 \n",
    "\n",
    "#dictionary size\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "# Word embedding dimensionality. \n",
    "# In general, higher dims means longer training, but increased accuracy\n",
    "EMBEDDING_DIM = 300 \n",
    "\n",
    "# scheme for calculating accuracy etc\n",
    "avg = 'weighted'\n",
    "\n",
    "# How much data to use. Set to 0 for no limits, useful for debugging\n",
    "DATA_LIMIT = 0 \n",
    "\n",
    "data_folder = '../data/'\n",
    "train_data = format(data_folder + \"train.csv\")\n",
    "val_data = format(data_folder + \"val.csv\")\n",
    "\n",
    "#store objects here\n",
    "model_folder = '../models_and_encoders/'\n",
    "\n",
    "#output csv to this folder\n",
    "results_folder = '../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(text):\n",
    "    \"\"\"\n",
    "    Does some cleaning by replacing any non-alphanummeric \n",
    "    content. Only useful for when the training data has \n",
    "    not already been cleaned. In reality, this cleaning is\n",
    "    too rigorous to be practical. \n",
    "    \n",
    "    Extra whitespace is also removed. \n",
    "    This allows for a quick trial of the network.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str) : a document to be cleaned\n",
    "    \n",
    "    Returns:\n",
    "        text (str) : document cleaned \n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z0-9-]\",\" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(texts):\n",
    "    \"\"\"\n",
    "    Takes an list of input texts and converts them \n",
    "    to a sequence of integers. The sequences are then padded\n",
    "    at the end, to assure that each is only <MAX_SEQUENCE_LENGTH> \n",
    "    words (or integers) long. Longer texts are shortened to \n",
    "    <MAX_SEQUENCE_LENGTH>. This number can be modified at the start\n",
    "    of the notebook.\n",
    "    \n",
    "    Parameters:\n",
    "        texts: a list of texts\n",
    "        \n",
    "    Returns:\n",
    "        sequences: a list of sequences of 16 integers long each\n",
    "    \"\"\"\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data: we just need this to look up the feature vector of every word we can encounter. \n",
    "# unsure whether to keep this in a function\n",
    "def preparedataset(data_path, data_limit=DATA_LIMIT, shuffle=False):\n",
    "    \"\"\"\n",
    "    This method loads in the data set from data path and splits it into a X (data) and y (labels).\n",
    "    \n",
    "    Arguably it needs to be refactored, because it's grown to be too long for a single function\n",
    "    \n",
    "    Parameters:\n",
    "        data_path: path to the data file\n",
    "        data_limit: int between 0 and len(data) to indicate how many rows should be returned\n",
    "        shuffle: boolean whether to shuffle the data or not. Not really needed if you shuffle the data during training. \n",
    "    Returns:\n",
    "        X: list of descriptions (strings)\n",
    "        y: list of labels (strings)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = [], []\n",
    "    i = 0\n",
    "    \n",
    "    #this reads in the data oneline at a time.\n",
    "    with open(data_path, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            label = line.split(\"|\")[0]\n",
    "            text = line.split(\"|\")[1]\n",
    "            #print(text)\n",
    "            \n",
    "            # texts are already cleaned, just split on |. \n",
    "            # hacky way to get around skipping the header\n",
    "            if i > 0:\n",
    "                label = \"0\" + label if len(label) % 2 != 0 else label #solves a string - int issue\n",
    "\n",
    "                #cleanup in case this is needed - (not for our data) \n",
    "                # perhaps make this faster via spacy or so?\n",
    "                text = cleanSentence(text)\n",
    "                X.append(text.split())\n",
    "                y.append(label[:6])\n",
    "\n",
    "            i = i+1 #temporary solution pending pipeline changes\n",
    "\n",
    "    X, y  = np.array(X), np.array(y)\n",
    "\n",
    "    # shuffle the data\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(len(y))\n",
    "        X,y = X[idx], y[idx]\n",
    "\n",
    "    # apply limit\n",
    "    if data_limit > 0:\n",
    "        X,y = X[:data_limit], y[:data_limit]\n",
    "    \n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCNN():\n",
    "    \"\"\"\n",
    "    Defines the CNN architecture as proposed in my thesis. For full details, please refer to the pdf found in this repository.\n",
    "    \n",
    "    The CNN consists out of 4 'channels' of different filter sizes which all respond to different n-grams (word level)\n",
    "    I apply Batch Normalization, which I havent seen in any paper for NLP but it seems too obvious so I doubt I'm the first\n",
    "    The flattening operation essentially spreads the tensor from (e.g.) 15 x 256 -> ,3840 so the data can be used by the fully\n",
    "    connected layers\n",
    "    \n",
    "    Eventually the channels are merged together, and another dense layer is used for learning more complex representations.\n",
    "    \n",
    "    Parameters:\n",
    "        -\n",
    "    Returns:\n",
    "        model: a CNN architecture compliant with Keras' API\n",
    "    \"\"\"\n",
    "    \n",
    "    # I removed all the memes I referenced here. \n",
    "    s = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(s)\n",
    "    # ---------------------- kernel size 2 ----------------------\n",
    "\n",
    "\n",
    "    x = Conv1D(256, 2, activation='relu')(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # ---------------------- kernel size 3 ----------------------\n",
    "    y = Conv1D(256, 3, activation='relu')(embedded_sequences)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(1024, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    \n",
    "    # ---------------------- kernel size 4 ----------------------\n",
    "    z = Conv1D(256, 4, activation='relu')(embedded_sequences)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Flatten()(z)\n",
    "    z = Dense(1024, activation='relu')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "\n",
    "    # ---------------------- kernel size 5 ----------------------\n",
    "    q = Conv1D(256, 5, activation='relu')(embedded_sequences)\n",
    "    q = BatchNormalization()(q)\n",
    "    q = Flatten()(q)\n",
    "    q = Dense(1024, activation='relu')(q)\n",
    "    q = BatchNormalization()(q)\n",
    "    \n",
    "    #tie them all together\n",
    "    merged = concatenate([x, y, z, q])    \n",
    "    \n",
    "    #merged gets another dense + BN stack\n",
    "    merged = Dense(1024, activation='relu')(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    # define our softmax output, automatically gets the number of classes from the labelencoder\n",
    "    # who needs automl? I don't.\n",
    "    preds = Dense(len(le.classes_), activation='softmax')(merged)\n",
    "\n",
    "    # needs to be updated to new API in TF 2\n",
    "    model = Model(inputs=s, output=preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the data\n",
    "texts, y = preparedataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show a couple of texts\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the labelencoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "le = le.fit(y)\n",
    "\n",
    "#fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now transform the labels\n",
    "labels = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the list of classes in our labelencoder\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to labes\n",
    "labels = to_categorical(np.asarray(labels), num_classes=len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "# tokenize our text now\n",
    "x_train_a = tokenizetext(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check, whats the shape of our training data now?\n",
    "np.shape(x_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the word embeddings, this consumes the keyedvectors and the npy file associated with it\n",
    "word_vectors = KeyedVectors.load(model_folder+'skipg_all_embeddings_march_01_full.kv')\n",
    "\n",
    "#construct the embeddings from our word vectors\n",
    "vocabulary_size=min(len(word_index)+1,MAX_NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=MAX_NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "\n",
    "# define the embedding layer \n",
    "# Set trainable = True to allow fine-tuning of embeddings!\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the labelencoder so we could hypothethically re-run this experiment\n",
    "dump(le, model_folder+experiment_name+'_labelencoder.joblib') \n",
    "\n",
    "#save the tokenizer as well\n",
    "dump(tokenizer, model_folder+experiment_name+'_tokenizer.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the model\n",
    "model = getCNN()\n",
    "\n",
    "#print a model summary (text)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping criteria, model serialization, and related functions\n",
    "callbacks = [ModelCheckpoint(filepath=model_folder+experiment_name+'.hdf5', monitor='val_acc', verbose=1, save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our optimizer\n",
    "opt = keras.optimizers.RMSprop(lr=0.0001)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - what is our original text?\n",
    "texts_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - what is our tokenized text?\n",
    "x_train_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a tokenized sentence and returns the original words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    \"\"\"\n",
    "    Reverse engineers the tokenized text so we can move from a tokenized text (integers) back to text\n",
    "    \n",
    "    Parameters:\n",
    "        - list of indices (usually a line from the training data that was tokenized)\n",
    "    \n",
    "    Returns:\n",
    "        - the words from the tokenized text\n",
    "    \"\"\"\n",
    "    # Creating a reverse dictionary\n",
    "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "    \n",
    "    # Look up words in the dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    \n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "detokenizedtext = list(map(sequence_to_text, [x_train_a[0]]))\n",
    "\n",
    "#sanity check - what is our de-tokenized (back and forth) text?\n",
    "print(detokenizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - show the label again\n",
    "le.inverse_transform(np.argmax([y_a[0]], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model. You can get coffee now, it'll be a while.\n",
    "history = model.fit(x_train_a, y_a,\n",
    "    batch_size=batch_size,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model as saved during training\n",
    "model = load_model(filepath=model_folder+experiment_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training Data', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the actual class labels (again, a bit of a sanity check)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load these again (start here if you want to predict with an older model)\n",
    "tokenizer = load(model_folder+experiment_name+'_tokenizer.joblib')\n",
    "\n",
    "le = load(model_folder+experiment_name+'_labelencoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model saved during training (on the first data set- so not including fine tuning)\n",
    "model = load_model(filepath='models_and_encoders/'+experiment_name+'.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the validation data\n",
    "X_v, y_val = preparedataset(validation_data, data_limit=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape of the validation data\n",
    "np.shape(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions\n",
    "print('Started gathering predictions..')\n",
    "predictions = model.predict(tokenizetext(X_v))\n",
    "\n",
    "#convert the one-hot encoded labesl\n",
    "predictions = le.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "#print the accuracy score\n",
    "print(accuracy_score(y_val, predictions))\n",
    "\n",
    "#add prediction results to dataframe\n",
    "print(f1_score(y_val, predictions, average=avg))\n",
    "print(recall_score(y_val, predictions, average=avg))\n",
    "print(precision_score(y_val, predictions, average=avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually inspect a sample\n",
    "X_v[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and see what the label should be\n",
    "y_val[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and check what was predicted by our system\n",
    "predictions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we can draw up a confusion matrix to see what classes go well and what absolutely goes awful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compute confusion matrix \n",
    "#cnf_matrix = confusion_matrix(y_val, predictions)\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure(figsize=(72, 64))\n",
    "#plot_confusion_matrix(cnf_matrix, classes=le.classes_,\n",
    "#                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "#plt.figure(figsize=(72, 64))\n",
    "#plot_confusion_matrix(cnf_matrix, classes=le.classes_, normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "#\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotdf = pd.DataFrame(data={'accuracy': accs, 'num_samples': counts})\n",
    "\n",
    "#plt.figure(figsize=(18, 9))\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "#plt.scatter(counts, accs, s=156)\n",
    "#plt.title('Accuracy versus number of training samples (log)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleindex = 222\n",
    "\n",
    "#manually inspect a sample\n",
    "print(X_v[sampleindex])\n",
    "\n",
    "# and see what the label should be\n",
    "print(y_val[sampleindex])\n",
    "\n",
    "#and check what was predicted by our system\n",
    "print(predictions[sampleindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the good part, since we can go back and evaluate against HS-4 and HS-2 as well, we can get a rough guess at\n",
    "# at performance at these levels.\n",
    "hs2preds = [pred[:2] for pred in predictions]\n",
    "hs2v = [pred[:2] for pred in y_val]\n",
    "print(\"HS-2 performance of this model: \"+ format(accuracy_score(hs2v, hs2preds)))\n",
    "\n",
    "hs4preds = [pred[:4] for pred in predictions]\n",
    "hs4v = [pred[:4] for pred in y_val]\n",
    "print(\"HS-4 performance of this model: \"+ format(accuracy_score(hs4v, hs4preds)))\n",
    "\n",
    "hs6preds = [pred[:6] for pred in predictions]\n",
    "hs6v = [pred[:6] for pred in y_val]\n",
    "print(\"HS-6 performance of this model: \"+ format(accuracy_score(hs6v, hs6preds)))\n",
    "\n",
    "print(\"\\n(HS-6 performance was \"+format(accuracy_score(y_val, predictions))+\")\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
